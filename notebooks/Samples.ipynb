{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec004248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e390e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./vrevals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f5b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e009ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 16:09:11 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from sampler.chat_completion_sampler import ChatCompletionSampler, DivFirstSampler\n",
    "from sampler.vllm_sampler import VLLMSampler\n",
    "from math_evaluator import MathEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc3422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_instruction_math(question, question_prompt_template=None, step_by_step=False, tokenizer=None, apply_chat_template=False):\n",
    "    if question_prompt_template is not None:\n",
    "        prompt = question_prompt_template.format(question)\n",
    "    else:\n",
    "        if not step_by_step:\n",
    "            prompt = (\n",
    "                'Please answer the following math question. '\n",
    "                'Provide your final answer in the format \\\\boxed{YOUR_ANSWER}.\\n\\n'\n",
    "                f'Question:\\n{question}\\n\\n'\n",
    "            )\n",
    "        else:\n",
    "            prompt = (\n",
    "                'Please answer the following math question. You should think step by step to solve it.\\n\\n'\n",
    "                'Provide your final answer in the format \\\\boxed{YOUR_ANSWER}.\\n\\n'\n",
    "                f'Question:\\n{question}\\n\\n'\n",
    "            )\n",
    "    if tokenizer is not None and apply_chat_template:\n",
    "        prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "    \n",
    "class Args:\n",
    "    dataset_name = \"gsm8k\"\n",
    "    split = \"test\"\n",
    "    k_list = [1,4,8,32]\n",
    "    subset_num = None\n",
    "    step_by_step_prompt = True\n",
    "    n_threads = 1\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6b8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'vrevals/runs/default/gsm8k.qwen-1.5b-inst' and its parent directories created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokenizer': {'pretrained_model_name_or_path': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
       "  'trust_remote_code': True},\n",
       " 'question_prompt_template': 'Can you solve the following math problem? {} Put your final answer within \\\\boxed{{}}.',\n",
       " 'sampler': {'class': 'VLLMSampler',\n",
       "  'model_name': 'nnheui/thinking_distilled-qwen2.5-1.5b-instruct-gsm8k',\n",
       "  'revision': 'step_50',\n",
       "  'temperature': 0.7,\n",
       "  'top_p': 1.0,\n",
       "  'top_k': -1,\n",
       "  'max_tokens': 6000}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_dir = Path(f\"vrevals/runs/default/{args.dataset_name}.qwen-1.5b-inst\")\n",
    "try:\n",
    "    job_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directory '{job_dir}' and its parent directories created successfully.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating directory: {e}\")\n",
    "    \n",
    "prompt_csv_path = f'{job_dir}/{args.split}.prompts.csv'\n",
    "sampler_config_dir = f'{job_dir}/distilled-50.direct/sample_2'\n",
    "\n",
    "with open(f\"{sampler_config_dir}/sampler_config.yaml\", \"r\") as f:\n",
    "    sampler_config = yaml.safe_load(f)\n",
    "sampler_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2976c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"tokenizer\" in sampler_config:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(**sampler_config['tokenizer'])\n",
    "# else:\n",
    "#     tokenizer = None\n",
    "    \n",
    "# eval = MathEval(args.dataset_name, \n",
    "#                 args.split, \n",
    "#                 args.k_list, \n",
    "#                 args.subset_num, \n",
    "#                 step_by_step_prompt=True,\n",
    "#                 n_threads=args.n_threads)\n",
    "\n",
    "# processed_prompt_data = []\n",
    "# for e in eval.examples:\n",
    "#     question = e[\"Question\"]\n",
    "#     e['prompt'] = get_task_instruction_math(question, \n",
    "#                                                sampler_config.get(\"question_prompt_template\"),\n",
    "#                                                tokenizer=tokenizer,\n",
    "#                                                apply_chat_template=True,\n",
    "#                                                step_by_step=True,)\n",
    "#     processed_prompt_data.append(\n",
    "#         (e['id'], e['id'], e['Question'], e['answer'], e['prompt'])\n",
    "#     )\n",
    "# prompt_df = pd.DataFrame(data=processed_prompt_data, columns=['question_id', 'prompt_id', 'question', 'answer', 'prompt'])\n",
    "# prompt_df.to_csv(prompt_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cef7aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you solve the following math problem? Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? Put your final answer within \\boxed{}.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokenizer': {'pretrained_model_name_or_path': 'Qwen/Qwen2.5-1.5B-Instruct',\n",
       "  'trust_remote_code': True},\n",
       " 'question_prompt_template': 'Can you solve the following math problem? {} Put your final answer within \\\\boxed{{}}.',\n",
       " 'sampler': {'class': 'VLLMSampler',\n",
       "  'model_name': 'nnheui/thinking_distilled-qwen2.5-1.5b-instruct-gsm8k',\n",
       "  'revision': 'step_50',\n",
       "  'temperature': 0.7,\n",
       "  'top_p': 1.0,\n",
       "  'top_k': -1,\n",
       "  'max_tokens': 6000}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df = pd.read_csv(prompt_csv_path)\n",
    "print(prompt_df['prompt'][0])\n",
    "with open(f\"{sampler_config_dir}/sampler_config.yaml\", \"r\") as f:\n",
    "    sampler_config = yaml.safe_load(f)\n",
    "sampler_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc8567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name_or_path': 'nnheui/thinking_distilled-qwen2.5-1.5b-instruct-gsm8k', 'revision': 'step_50', 'temperature': 0.7, 'top_p': 1.0, 'top_k': -1, 'max_tokens': 6000}\n"
     ]
    }
   ],
   "source": [
    "# Extract tokenizer config and sampler config\n",
    "tokenizer_config = sampler_config.get(\"tokenizer\", {})\n",
    "sampler_config_section = sampler_config.get(\"sampler\", {})\n",
    "\n",
    "# Dynamically load the sampler class\n",
    "sampler_class_name = sampler_config_section.get(\"class\", \"ChatCompletionSampler\")\n",
    "sampler_classes = {\n",
    "    \"ChatCompletionSampler\": ChatCompletionSampler,\n",
    "    \"DivFirstSampler\": DivFirstSampler,\n",
    "    \"VLLMSampler\": VLLMSampler,\n",
    "}\n",
    "# SamplerClass = sampler_classes[sampler_class_name]\n",
    "SamplerClass = VLLMSampler\n",
    "\n",
    "# Remove keys that are not arguments to SamplerClass.__init__\n",
    "init_args = {\n",
    "    # \"api_key_name\": \"VLLM_TOKEN\",\n",
    "    # \"base_url\": f\"http://localhost:{port}/v1\",\n",
    "}\n",
    "for k, v in sampler_config_section.items():\n",
    "    if k == \"class\":\n",
    "        continue\n",
    "    # Renaming config keys to match the argument names where needed\n",
    "    if k == \"model_name\":\n",
    "        # init_args[\"model\"] = v\n",
    "        init_args[\"model_name_or_path\"] = v\n",
    "    # elif k == \"api_key_name\": \n",
    "    #     continue\n",
    "    else:\n",
    "        init_args[k] = v\n",
    "    \n",
    "print(init_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a7b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 16:09:21 [utils.py:233] non-default args: {'disable_log_stats': True, 'revision': 'step_50', 'model': 'nnheui/thinking_distilled-qwen2.5-1.5b-instruct-gsm8k'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 16:09:22 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 16:09:22 [model.py:1510] Using max model len 32768\n",
      "INFO 11-22 16:09:23 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:24 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:24 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='nnheui/thinking_distilled-qwen2.5-1.5b-instruct-gsm8k', speculative_config=None, tokenizer='nnheui/thinking_distilled-qwen2.5-1.5b-instruct-gsm8k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=step_50, tokenizer_revision=step_50, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=nnheui/thinking_distilled-qwen2.5-1.5b-instruct-gsm8k, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:27 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m WARNING 11-22 16:09:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:27 [gpu_model_runner.py:2602] Starting to load model nnheui/thinking_distilled-qwen2.5-1.5b-instruct-gsm8k...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:28 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:28 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:28 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:28 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f443840e295452cb1e336c920f6f4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:30 [default_loader.py:267] Loading weights took 1.16 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:30 [gpu_model_runner.py:2653] Model loading took 2.8876 GiB and 1.832769 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:36 [backends.py:548] Using cache directory: /home/grads/hnn5071/.cache/vllm/torch_compile_cache/12371887f6/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:36 [backends.py:559] Dynamo bytecode transform time: 5.54 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:38 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.603 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:39 [monitor.py:34] torch.compile takes 5.54 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:40 [gpu_worker.py:298] Available KV cache memory: 38.44 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:41 [kv_cache_utils.py:1087] GPU KV cache size: 1,439,664 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:41 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 43.94x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 26.97it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 23.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:45 [gpu_model_runner.py:3480] Graph capturing finished in 5 secs, took 0.62 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3996937)\u001b[0;0m INFO 11-22 16:09:45 [core.py:210] init engine (profile, create kv cache, warmup model) took 15.09 seconds\n",
      "INFO 11-22 16:09:47 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Create the sampler\n",
    "sampler = SamplerClass(**init_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43c7c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_config['sampler']['max_tokens'] = 8000\n",
    "# sampler.max_tokens = sampler_config['sampler']['max_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e087e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you solve the following math problem? Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? Put your final answer within \\boxed{}.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = prompt_df['prompt'].apply(lambda x: x + \"<think>\\n\")\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affe5532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458db5d48c44dd281d9d60aa7fb2ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa05a70315f4f38b21abe73fe85810a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1319 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = sampler.complete(prompts, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08894143",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = []\n",
    "for res, (_, row) in zip(response, prompt_df.iterrows()):\n",
    "    for out in res.choices:\n",
    "        generations.append((row['question_id'], row['prompt_id'], out.response_text, None, row['answer'], sampler_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b61209",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df = pd.DataFrame(data=generations, columns=['question_id', 'prompt_id', 'response', 'pred_answer', 'gt_answer', 'sampler_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1dfcf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>response</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>gt_answer</th>\n",
       "      <th>sampler_config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay, so I've got this math problem that Janet...</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Okay, so I have this math problem here: \"A rob...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Okay, so I have this math problem to solve, an...</td>\n",
       "      <td>None</td>\n",
       "      <td>70000</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Okay, so James is running 3 sprints each day, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>540</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Okay, so I've got this math problem here about...</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>1314</td>\n",
       "      <td>1314</td>\n",
       "      <td>Okay, let's see. So, I have this math problem ...</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>1315</td>\n",
       "      <td>1315</td>\n",
       "      <td>Okay, let me try to figure out this math probl...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>1316</td>\n",
       "      <td>1316</td>\n",
       "      <td>Okay, so Mark needs a new radiator for his car...</td>\n",
       "      <td>None</td>\n",
       "      <td>230</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>1317</td>\n",
       "      <td>1317</td>\n",
       "      <td>Okay, so I've got this math problem here: Farm...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>1318</td>\n",
       "      <td>1318</td>\n",
       "      <td>Okay, so I have this math problem here: Henry ...</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1319 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      question_id  prompt_id  \\\n",
       "0               0          0   \n",
       "1               1          1   \n",
       "2               2          2   \n",
       "3               3          3   \n",
       "4               4          4   \n",
       "...           ...        ...   \n",
       "1314         1314       1314   \n",
       "1315         1315       1315   \n",
       "1316         1316       1316   \n",
       "1317         1317       1317   \n",
       "1318         1318       1318   \n",
       "\n",
       "                                               response pred_answer  \\\n",
       "0     Okay, so I've got this math problem that Janet...        None   \n",
       "1     Okay, so I have this math problem here: \"A rob...        None   \n",
       "2     Okay, so I have this math problem to solve, an...        None   \n",
       "3     Okay, so James is running 3 sprints each day, ...        None   \n",
       "4     Okay, so I've got this math problem here about...        None   \n",
       "...                                                 ...         ...   \n",
       "1314  Okay, let's see. So, I have this math problem ...        None   \n",
       "1315  Okay, let me try to figure out this math probl...        None   \n",
       "1316  Okay, so Mark needs a new radiator for his car...        None   \n",
       "1317  Okay, so I've got this math problem here: Farm...        None   \n",
       "1318  Okay, so I have this math problem here: Henry ...        None   \n",
       "\n",
       "      gt_answer                                     sampler_config  \n",
       "0            18  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "1             3  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "2         70000  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "3           540  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "4            20  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "...         ...                                                ...  \n",
       "1314          8  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "1315          5  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "1316        230  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "1317          5  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "1318         14  {'tokenizer': {'pretrained_model_name_or_path'...  \n",
       "\n",
       "[1319 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98dc02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.localtime()\n",
    "generation_csv_name = f'generations.{t.tm_mon}.{t.tm_mday},{t.tm_hour}:{t.tm_min}.csv'\n",
    "gen_df.to_csv(f\"{sampler_config_dir}/{generation_csv_name}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48f789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
