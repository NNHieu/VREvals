{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8cbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./vrevals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fb5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960cfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from utils.math_equivalence import is_equiv\n",
    "from utils.pass_k_utils import estimate_pass_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e966091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_math_answer(output, mode='gen'):\n",
    "    if \"</think>\" in output:\n",
    "        output = output.split(\"</think>\")[1]\n",
    "    extracted_text = ''\n",
    "    # Existing extraction logic for 'gen' and 'choose' modes\n",
    "    pattern = r'\\\\boxed\\{(.*)\\}'\n",
    "    matches = re.findall(pattern, output)\n",
    "    if matches:\n",
    "        extracted_text = matches[-1]  # Take the last match\n",
    "        if mode in ['choose', 'qa']:\n",
    "            # Handle 'choose' mode\n",
    "            inner_pattern = r'\\\\text\\{(.*)\\}'\n",
    "            inner_matches = re.findall(inner_pattern, extracted_text)\n",
    "            if inner_matches:\n",
    "                extracted_text = inner_matches[-1]  # Take the last match\n",
    "            extracted_text = extracted_text.strip(\"()\")\n",
    "    return extracted_text\n",
    "\n",
    "def normalize_answer(text):\n",
    "    text = text.lower()\n",
    "    text = \" \".join(text.strip().split())\n",
    "    return text\n",
    "\n",
    "def evaluate_predictions(pred_answer, labeled_answer):\n",
    "    final_metric = {\"is_valid_answer\": False, \"acc\": 0, \"em\": 0, \"f1\": 0, 'math_equal': 0}\n",
    "    if pred_answer != '':\n",
    "        final_metric[\"is_valid_answer\"] = True\n",
    "\n",
    "    normalized_pred_answer = normalize_answer(pred_answer)\n",
    "    normalized_ground_truth = normalize_answer(labeled_answer)\n",
    "\n",
    "    em = int(normalized_pred_answer == normalized_ground_truth)\n",
    "    acc = int(normalized_ground_truth in normalized_pred_answer)\n",
    "\n",
    "    prediction_tokens = normalized_pred_answer.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        precision = 1.0 * num_same / len(prediction_tokens) if len(prediction_tokens) > 0 else 0\n",
    "        recall = 1.0 * num_same / len(ground_truth_tokens) if len(ground_truth_tokens) > 0 else 0\n",
    "        if (precision + recall) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    final_metric[\"em\"] = em\n",
    "    final_metric[\"acc\"] = acc\n",
    "    final_metric[\"f1\"] = f1\n",
    "\n",
    "    final_metric[\"math_equal\"] = is_equiv(normalized_pred_answer, normalized_ground_truth)\n",
    "\n",
    "    # print(em, acc, f1, normalized_pred_answer, '|', normalized_ground_truth)\n",
    "    return final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "95201fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dataset_name = \"gsm8k\"\n",
    "    split = \"test\"\n",
    "    k_list = [1,4,8,32]\n",
    "    subset_num = None\n",
    "    step_by_step_prompt = True\n",
    "    n_threads = 1\n",
    "args = Args()\n",
    "job_dir = f\"vrevals/runs/default/{args.dataset_name}.qwen-1.5b-inst\"\n",
    "prompt_csv_path = f'{job_dir}/{args.split}.prompts.csv'\n",
    "# sampler_config_dir = f'{job_dir}/distilled-50.direct/sample_1'\n",
    "# sampler_config_dir = f'{job_dir}/distilled-50.direct/sample_2'\n",
    "\n",
    "# sampler_config_dir = f'{job_dir}/distilled-50.direct/sample_*'\n",
    "sampler_config_dir = f'{job_dir}/distilled-50.direct/sample_[123]'\n",
    "# sampler_config_dir = f'{job_dir}/direct/sample_*'\n",
    "\n",
    "# sampler_config_dir = f'{job_dir}/direct/sample_1'\n",
    "# with open(f\"{sampler_config_dir}/sampler_config.yaml\", \"r\") as f:\n",
    "#     sampler_config = yaml.safe_load(f)\n",
    "# sampler_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4408c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generation_csv = glob.glob(f\"{sampler_config_dir}/generations.*.csv\")\n",
    "all_generation_df = [pd.read_csv(p) for p in all_generation_csv]\n",
    "# Concatenate all dataframes in all_generation_df into a single dataframe\n",
    "generation_df = pd.concat(all_generation_df, ignore_index=True)\n",
    "generation_df['pred_answer'] = generation_df.response.apply(extract_math_answer)\n",
    "metrics = generation_df.apply(lambda x: evaluate_predictions(str(x['pred_answer']), str(x['gt_answer'])), axis=1)\n",
    "generation_df['is_correct'] = [e['math_equal'] for e in metrics]\n",
    "generation_df['is_valid'] = [e['is_valid_answer'] for e in metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "13951784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>response</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>gt_answer</th>\n",
       "      <th>sampler_config</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay, so Janet's ducks lay 16 eggs each day. H...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Okay, so I need to figure out how much Janet m...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Okay, let's see. I have this math problem here...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Okay, so I need to solve this math problem. Le...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Okay, so Josh is trying to make some money by ...</td>\n",
       "      <td>70000</td>\n",
       "      <td>70000</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17142</th>\n",
       "      <td>1314</td>\n",
       "      <td>1314</td>\n",
       "      <td>Okay, so I need to solve this math problem ste...</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17143</th>\n",
       "      <td>1315</td>\n",
       "      <td>1315</td>\n",
       "      <td>Okay, so I have this math problem here about o...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17144</th>\n",
       "      <td>1316</td>\n",
       "      <td>1316</td>\n",
       "      <td>Okay, so Mark's car broke down, and he needs a...</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17145</th>\n",
       "      <td>1317</td>\n",
       "      <td>1317</td>\n",
       "      <td>Okay, so Farmer Brown has 20 animals on his fa...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17146</th>\n",
       "      <td>1318</td>\n",
       "      <td>1318</td>\n",
       "      <td>Okay, so I have this math problem here: Henry ...</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>{'tokenizer': {'pretrained_model_name_or_path'...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17147 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_id  prompt_id  \\\n",
       "0                0          0   \n",
       "1                0          0   \n",
       "2                1          1   \n",
       "3                1          1   \n",
       "4                2          2   \n",
       "...            ...        ...   \n",
       "17142         1314       1314   \n",
       "17143         1315       1315   \n",
       "17144         1316       1316   \n",
       "17145         1317       1317   \n",
       "17146         1318       1318   \n",
       "\n",
       "                                                response pred_answer  \\\n",
       "0      Okay, so Janet's ducks lay 16 eggs each day. H...          18   \n",
       "1      Okay, so I need to figure out how much Janet m...          18   \n",
       "2      Okay, let's see. I have this math problem here...           3   \n",
       "3      Okay, so I need to solve this math problem. Le...           3   \n",
       "4      Okay, so Josh is trying to make some money by ...       70000   \n",
       "...                                                  ...         ...   \n",
       "17142  Okay, so I need to solve this math problem ste...               \n",
       "17143  Okay, so I have this math problem here about o...           5   \n",
       "17144  Okay, so Mark's car broke down, and he needs a...         230   \n",
       "17145  Okay, so Farmer Brown has 20 animals on his fa...           5   \n",
       "17146  Okay, so I have this math problem here: Henry ...          14   \n",
       "\n",
       "       gt_answer                                     sampler_config  \\\n",
       "0             18  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "1             18  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "2              3  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "3              3  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "4          70000  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "...          ...                                                ...   \n",
       "17142          8  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "17143          5  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "17144        230  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "17145          5  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "17146         14  {'tokenizer': {'pretrained_model_name_or_path'...   \n",
       "\n",
       "       is_correct  is_valid  \n",
       "0            True      True  \n",
       "1            True      True  \n",
       "2            True      True  \n",
       "3            True      True  \n",
       "4            True      True  \n",
       "...           ...       ...  \n",
       "17142       False     False  \n",
       "17143        True      True  \n",
       "17144        True      True  \n",
       "17145        True      True  \n",
       "17146        True      True  \n",
       "\n",
       "[17147 rows x 8 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "685e123d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pass@1': np.float64(0.8994576310724907),\n",
       " 'pass@2': np.float64(0.9770124997570032),\n",
       " 'pass@3': np.float64(0.9922700498894585),\n",
       " 'pass@4': np.float64(0.9967224587391381),\n",
       " 'pass@5': np.float64(0.9983334835495562),\n",
       " 'pass@6': np.float64(0.9990156419269384),\n",
       " 'pass@7': np.float64(0.9993536284286854)}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df = generation_df.groupby(['question_id', 'prompt_id'])\n",
    "\n",
    "valid_grouped = grouped_df.is_valid.apply(list).reset_index(name='valids')\n",
    "valid_grouped['num_samples'] = valid_grouped.valids.apply(len)\n",
    "valid_grouped['num_math_equal'] = valid_grouped.valids.apply(sum)\n",
    "\n",
    "min_num_samples = valid_grouped['num_samples'].min()\n",
    "k_list = [1,2,3,4,5,6,7]\n",
    "detail_pass_at_k = {\n",
    "    f\"pass@{k}\": estimate_pass_at_k(valid_grouped['num_samples'].values, \n",
    "                                    valid_grouped['num_math_equal'].values, k)\n",
    "    for k in k_list\n",
    "    if (min_num_samples >= k).all()\n",
    "}\n",
    "pass_at_k = {k: detail_pass_at_k[k].mean() for k in detail_pass_at_k}\n",
    "pass_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "941b436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'pass@1': np.float64(0.9086429112964367),\n",
    "#  'pass@2': np.float64(0.9840788476118272)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "865a9fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pass@1': np.float64(0.697746359023842)}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataframe to keep only rows where the response is valid\n",
    "valid_generation_df = generation_df[generation_df['is_valid']]\n",
    "\n",
    "grouped_df = valid_generation_df.groupby(['question_id', 'prompt_id'])\n",
    "corrects_grouped = grouped_df.is_correct.apply(list).reset_index(name='corrects')\n",
    "corrects_grouped['num_samples'] = corrects_grouped.corrects.apply(len)\n",
    "corrects_grouped['num_math_equal'] = corrects_grouped.corrects.apply(sum)\n",
    "\n",
    "min_num_samples = corrects_grouped['num_samples'].min()\n",
    "\n",
    "k_list = [1,2,3,4,5,6,7]\n",
    "detail_pass_at_k = {\n",
    "    f\"pass@{k}\": estimate_pass_at_k(corrects_grouped['num_samples'].values, \n",
    "                                    corrects_grouped['num_math_equal'].values, k)\n",
    "    for k in k_list\n",
    "    if (min_num_samples >= k).all()\n",
    "}\n",
    "pass_at_k = {k: detail_pass_at_k[k].mean() for k in detail_pass_at_k}\n",
    "pass_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6324f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'pass@1': np.float64(0.694953077837839),\n",
    "#  'pass@2': np.float64(0.8097839655079988)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03063d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results = {\n",
    "    'detail_pass_at_k': {k:v.tolist() for k,v in detail_pass_at_k.items()},\n",
    "    'pass_at_k': pass_at_k,\n",
    "}\n",
    "final_metrics = {'overall': overall_results}\n",
    "t = time.localtime()\n",
    "metrics_json_name = f'metrics.{t.tm_mon}.{t.tm_mday},{t.tm_hour}:{t.tm_min}.json'\n",
    "with open(os.path.join(sampler_config_dir, metrics_json_name), mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(final_metrics, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf1d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
